def unit4():
    print('''
    Conjoint Analysis					
Conjoint analysis attempts to determine the relative importance consumers attach to salient attributes and the utilities they attach to the levels of attributes.  					
The respondents are presented with stimuli that consist of combinations of attribute levels and asked to evaluate these stimuli in terms of their desirability.  					
Conjoint procedures attempt to assign values to the levels of each attribute, so that the resulting values or utilities attached to the stimuli match, as closely as possible, the input evaluations provided by the respondents. 					
					
Statistics and Terms Associated with Conjoint Analysis					
Part-worth functions. The part-worth functions, or utility functions, describe the utility consumers attach to the levels of each attribute. 					
Relative importance weights. The relative importance weights are estimated and indicate which attributes are important in influencing consumer choice. 					
Attribute levels. The attribute levels denote the values assumed by the attributes. 					
Full profiles. Full profiles, or complete profiles of brands, are constructed in terms of all the attributes by using the attribute levels specified by the design. 					
Pairwise tables. In pairwise tables, the respondents evaluate two attributes at a time until all the required pairs of attributes have been evaluated. 					
					
Conducting Conjoint Analysis: Construct the Stimuli					
In the pairwise approach, also called two-factor evaluations, the respondents evaluate two attributes at a time until all the possible pairs of attributes have been evaluated.  					
In the full-profile approach, also called multiple-factor evaluations, full or complete profiles of brands are constructed for all the attributes.  Typically, each profile is described on a separate index card.  					
In the pairwise approach, it is possible to reduce the number of paired comparisons by using cyclical designs.  Likewise, in the full-profile approach, the number of stimulus profiles can be greatly reduced by means of fractional factorial designs.  					
A special class of fractional designs, called orthogonal arrays, allow for the efficient estimation of all main effects.  Orthogonal arrays permit the measurement of all main effects of interest on an uncorrelated basis.  These designs assume that all interactions are negligible.  					
Generally, two sets of data are obtained.  One, the estimation set, is used to calculate the part-worth functions for the attribute levels.  The other, the holdout set, is used to assess reliability and validity.					
					
Conducting Conjoint Analysis: Decide on the Form of Input Data 					
For non-metric data, the respondents are typically required to provide rank-order evaluations.  					
In the metric form, the respondents provide ratings, rather than rankings.  In this case, the judgments are typically made independently.  					
In recent years, the use of ratings has become increasingly common. 					
The dependent variable is usually preference or intention to buy.  However, the conjoint methodology is flexible and can accommodate a range of other dependent variables, including actual purchase or choice. 					
In evaluating sneaker profiles, respondents were required to provide preference. 					
					
Conducting Conjoint Analysis: Interpret the Results					
For interpreting the results, it is helpful to plot the part-worth functions.					
The utility values have only interval scale properties, and their origin is arbitrary.  					
The relative importance of attributes should be considered. 					
					
Conducting Conjoint Analysis: Assessing Reliability and Validity					
The goodness of fit of the estimated model should be evaluated.  For example, if dummy variable regression is used, the value of R2 will indicate the extent to which the model fits the data.  					
Test-retest reliability can be assessed by obtaining a few replicated judgments later in data collection.					
The evaluations for the holdout or validation stimuli can be predicted by the estimated part-worth functions.  The predicted evaluations can then be correlated with those obtained from the respondents to determine internal validity.					
If an aggregate-level analysis has been conducted, the estimation sample can be split in several ways and conjoint analysis conducted on each subsample.  The results can be compared across subsamples to assess the stability of conjoint analysis solutions. 					
					
Assumptions and Limitations of Conjoint Analysis					
Conjoint analysis assumes that the important attributes of a product can be identified.  					
It assumes that consumers evaluate the choice alternatives in terms of these attributes and make tradeoffs.  					
The tradeoff model may not be a good representation of the choice process.  					
Another limitation is that data collection may be complex, particularly if a large number of attributes are involved and the model must be estimated at the individual level.  					
The part-worth functions are not unique. 					
					
Consumer preference in Carpet Cleaning					
Which attributes have the most influence on the customer’s likelihood of purchase? (Maximum Spread)					
Within each attribute, which level has highest rank?  (Draw utility charts)					
					
Using conjoint analysis to segment the market					
Say you have sample of 100 customers					
Construct a regression equation for each customer as earlier					
Use regression coefficients for each customer as row and perform cluster analysis					
E.g. in example (-4.5,3.5,-1.5,-2,7.67,4.83,1.5,4.5) would be a row for a customer.					
					
What is Factor Analysis?					
Factor analysis is a multivariate statistical technique in which there is no distinction between dependent and independent variables.  i.e. It is interdependence technique					
In factor analysis, all variables under investigation are analysed together to extract the underlined factors. 					
Factor analysis is a data reduction method. 					
It is a very useful method to reduce a large number of variables resulting in data complexity to a few manageable factors. 					
These factors explain most part of the variations of the original set of data.					
A factor is a linear combination of variables. 					
It is a construct that is not directly observable but that needs to be inferred from the input variables. 					
The factors are statistically independent.					
					
Uses of Factor Analysis					
Scale construction: Factor analysis could be used to develop concise multiple item scales for measuring various constructs.					
Establish antecedents: This method reduces multiple input variables into grouped factors. Thus, the independent variables can be grouped into broad factors. 					
Psychographic profiling: Different independent variables are grouped to measure independent factors. These are then used for identifying personality types.					
Segmentation analysis: Factor analysis could also be used for segmentation. For example, there could be different sets of two-wheelers-customers owning two-wheelers because of different importance they give to factors like prestige, economy consideration and functional features.					
Marketing studies: The technique has extensive use in the field of marketing and can be successfully used for new product development; product acceptance research, developing of advertising copy, pricing studies and for branding studies. 					
For example we can use it to:					
identify the attributes of brands that influence consumers’ choice;					
 get an insight into the media habits of various consumers;					
 identify the characteristics of price-sensitive customers.					
					
Factor analysis is used in the following circumstances: 					
To identify underlying dimensions, or factors, that explain the correlations among a set of variables.  					
To identify a new, smaller, set of uncorrelated variables to replace the original set of correlated variables in subsequent multivariate analysis (regression or discriminant analysis).  					
To identify a smaller set of salient variables from a larger set for use in subsequent multivariate analysis.					
					
Factor Analysis Model					
Mathematically, each variable is expressed as a linear combination of underlying factors.  The covariation among the variables is described in terms of a small number of common factors plus a unique factor for each variable.  If the variables are standardized,  the factor analysis model may be represented as:					
					
	Xi = Ai 1F1 + Ai 2F2 + Ai 3F3 + . . . + AimFm + ViUi				
 					
	where,				
 					
	Xi 	=	i th standardized variable		
	Aij	= 	standardized multiple regression coefficient of 		
			variable i on common factor j		
	F 	=	common factor		
	Vi	=	standardized regression coefficient of variable i on 		unique factor i
	Ui 	=	the unique factor for variable i		
	m 	=	number of common factors		
					
The unique factors are uncorrelated with each other and with the common factors.  The common factors themselves can be expressed as linear combinations of the observed variables.					
					
	Fi = Wi1X1 + Wi2X2 + Wi3X3 + . . . + WikXk				
 					
	Where:				
 					
	Fi 	=	estimate of i th factor		
	Wi 	=	weight or factor score coefficient		
	k 	=	number of variables 		
					
Statistics Associated with Factor Analysis					
Bartlett's test of sphericity. Bartlett's test of sphericity is a test statistic used to examine the hypothesis that the variables are uncorrelated in the population.  In other words, the population correlation matrix is an identity matrix; each variable correlates perfectly with itself (r = 1) but has no correlation with the other variables (r = 0). 					
Correlation matrix. A correlation matrix is a lower triangle matrix showing the simple correlations, r, between all possible pairs of variables included in the analysis.  The diagonal elements, which are all 1, are usually omitted. 					
					
Communality. Communality is the amount of variance a variable shares with all the other variables being considered.  This is also the proportion of variance explained by the common factors. 					
Eigenvalue. The eigenvalue represents the total variance explained by each factor. The eigenvalue of any factor is obtained by taking the sum of squares of the factor loadings of each component.					
Factor loadings. Factor loadings are simple correlations between the variables and the factors. 					
Factor loading plot. A factor loading plot is a plot of the original variables using the factor loadings as coordinates. 					
Factor matrix. A factor matrix contains the factor loadings of all the variables on all the factors extracted. 					
Factor scores.  Factor scores are composite scores estimated for each respondent on the derived factors. 					
Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy. The Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy is an index used to examine the appropriateness of factor analysis.  High values (between 0.5 and 1.0) indicate factor analysis is appropriate.  Values below 0.5 imply that factor analysis may not be appropriate. 					
Percentage of variance. The percentage of the total variance attributed to each factor. 					
Residuals are the differences between the observed correlations, as given in the input correlation matrix, and the reproduced correlations, as estimated from the factor matrix. 					
Scree plot. A scree plot is a plot of the Eigenvalues against the number of factors in order of extraction. 					
					
Conditions for a Factor Analysis Exercise					
The following conditions must be ensured before executing the technique:					
Factor analysis exercise requires metric data. This means the data should be either interval or ratio scale in nature. 					
The variables for factor analysis are identified through exploratory research which may be conducted by reviewing the literature on the subject, researches carried out already in this area, by informal interviews of knowledgeable persons, qualitative analysis like focus group discussions held with a small sample of the respondent population, analysis of case studies and judgement of the researcher.					
As the responses to different statements are obtained through different scales, all the responses need to be standardized. The standardization helps in comparison of different responses from such scales.					
The size of the sample respondents should be at least four to five times more than the number of variables (number of statements).					
The basic principle behind the application of factor analysis is that the initial set of variables should be highly correlated. If the correlation coefficients between all the variables are small, factor analysis may not be an appropriate technique.					
The significance of correlation matrix is tested using Bartlett’s test of sphericity.  The hypothesis to be tested is  					
H0 : Correlation matrix is insignificant, i.e., correlation matrix is an identity matrix where diagonal elements are one and off diagonal elements are zero.					
H1 : Correlation matrix is significant.					
					
The test converts it into a chi-square statistics with degrees of freedom equal to [(k(k-1))/2], where k is the number of variables on which factor analysis is applied. The significance of the correlation matrix ensures that a factor analysis exercise could be carried out.					
The value of Kaiser-Meyer-Olkin (KMO) statistics which takes a value between 0 and 1 should be greater than 0.5 for the application of factor analysis. 					
The KMO statistics compares the magnitude of observed correlation coefficients with the magnitudes of partial correlation coefficients. 					
A small value of KMO shows that correlation between variables cannot be explained by other variables.					
''')


unit4()

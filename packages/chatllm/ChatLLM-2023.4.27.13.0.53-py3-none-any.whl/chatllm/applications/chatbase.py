#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Project      : AI.  @by PyCharm
# @File         : Question2Answer
# @Time         : 2023/4/21 12:25
# @Author       : betterme
# @WeChat       : meutils
# @Software     : PyCharm
# @Description  :
import types
from meutils.pipe import *
from meutils.decorators import clear_cuda_cache

from chatllm.utils import DEVICE, load_llm4chat


class ChatBase(object):

    def __init__(self, chat_func=None, prompt_template=None,
                 role='ä½ æ‰®æ¼”çš„è§’è‰²æ˜¯ChatLLMæ™ºèƒ½æœºå™¨äººğŸ¤–ï¼Œç”±BettermeäºŒæ¬¡å¼€å‘'):
        self.chat_func = chat_func
        self.prompt_template = prompt_template if prompt_template else self.default_document_prompt
        self.role = role

        #
        self.history = []
        self.knowledge_base = None

    def __call__(self, **kwargs):
        return self.qa(**kwargs)

    def qa(self, query, knowledge_base='è¯·è‡ªç”±å›ç­”', **kwargs):
        """å¯é‡å†™"""
        return self._qa(query, knowledge_base, **kwargs)

    def set_chat_kwargs(self, **kwargs):
        self.chat_func = partial(self.chat_func, **kwargs)

    @clear_cuda_cache
    def _qa(self, query, knowledge_base='', max_turns=1):
        self.knowledge_base = knowledge_base if knowledge_base.strip() else 'è¯·è‡ªç”±å›ç­”'

        query = self.prompt_template.format(context=self.knowledge_base, question=query, role=self.role)

        _history = self.history[-(max_turns - 1):] if max_turns > 1 else []
        result = self.chat_func(query=query, history=_history)

        if isinstance(result, types.GeneratorType):
            return self._stream(result)
        else:  # list(self._stream(result)) æƒ³åŠæ³•åˆå¹¶
            response, history = result
            # self.history_ = history  # å†å²æ‰€æœ‰
            self.history += [[None, response]]  # ç½®ç©ºçŸ¥è¯†
            return result  # response, history

    def _stream(self, result):  # yield > return
        response = None
        bar = tqdm(result, ascii=True)  # ncols
        for response, history in bar:
            bar.set_description(response)
            yield response, history
        # self.history_ = history  # å†å²æ‰€æœ‰
        self.history += [[None, response]]  # ç½®ç©ºçŸ¥è¯†

    def load_llm4chat(self, model_name_or_path="THUDM/chatglm-6b", device=DEVICE, stream=True, **kwargs):
        assert not self.chat_func, "overwrite chat_func"
        self.chat_func = load_llm4chat(model_name_or_path, device, stream, **kwargs)

    @property
    def default_document_prompt(self):
        prompt_template = """
            {role} åŸºäºä»¥ä¸‹å·²çŸ¥ä¿¡æ¯ï¼Œç®€æ´å’Œä¸“ä¸šçš„æ¥å›ç­”é—®é¢˜ã€‚
            å¦‚æœæ— æ³•ä»ä¸­å¾—åˆ°ç­”æ¡ˆï¼Œè¯·è¯´ "æ ¹æ®å·²çŸ¥ä¿¡æ¯æ— æ³•å›ç­”è¯¥é—®é¢˜" æˆ– "æ²¡æœ‰æä¾›è¶³å¤Ÿçš„ä¿¡æ¯"ï¼Œä¸å…è®¸åœ¨ç­”æ¡ˆä¸­æ·»åŠ ç¼–é€ æˆåˆ†ï¼Œç­”æ¡ˆè¯·ä½¿ç”¨ä¸­æ–‡ã€‚
            å·²çŸ¥ä¿¡æ¯: {context}
            é—®é¢˜: {question}
            """.strip()

        return prompt_template


if __name__ == '__main__':
    from chatllm.applications import ChatBase

    qa = ChatBase()
    qa.load_llm4chat(model_name_or_path="/Users/betterme/PycharmProjects/AI/CHAT_MODEL/chatglm")

    for i, _ in qa(query='å‘¨æ°ä¼¦æ˜¯è°', knowledge_base='å‘¨æ°ä¼¦æ˜¯å‚»å­'):
        pass

    for i, _ in qa(query='ä½ æ˜¯è°', knowledge_base='è‡ªç”±å›ç­”'):
        pass

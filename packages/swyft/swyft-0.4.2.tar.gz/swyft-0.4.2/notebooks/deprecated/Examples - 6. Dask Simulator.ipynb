{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLcCZoN9Ln9X"
   },
   "source": [
    "# Using Dask to parallelize simulations\n",
    "\n",
    "In real-world use, `SWYFT` will ofen be combined with existing simulators. As inference using truncated marginal neural ratio estimation (as implemented in `SWYFT`), allows the reuse **and** independent realization of simulations, SWYFT makes the use of more computationally expensive and (physically) relevant simulators viable. \n",
    "\n",
    "In `SWYFT` this supported by the use of the directory store, but even more importantly by the `DaskSimulator` class. In contrast to the `Simulator` class, `DaskSimulator` allows the simulations requested to executed in parallel on a Dask cluster.\n",
    "\n",
    "In this notebook we demonstrate the framework for the use of a more computationally expensive external simulator, although we still make use of the toy external simulator we previously defined.\n",
    "\n",
    "Here we have chosen to include all explanatory text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZGCjgra6z91",
    "outputId": "0a94039f-de82-4799-8d95-7019d65977b3"
   },
   "outputs": [],
   "source": [
    "# DON'T FORGET TO ACTIVATE THE GPU when on google colab (Edit > Notebook settings)\n",
    "from os import environ\n",
    "GOOGLE_COLAB = True if \"COLAB_GPU\" in environ else False\n",
    "if GOOGLE_COLAB:\n",
    "    !pip install git+https://github.com/undark-lab/swyft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F4qPfUYOE0YY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pylab as plt\n",
    "import os\n",
    "\n",
    "import swyft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DgK78OA5FKVt"
   },
   "outputs": [],
   "source": [
    "# Set randomness\n",
    "np.random.seed(25)\n",
    "torch.manual_seed(25)\n",
    "\n",
    "# cwd\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# swyft\n",
    "device = 'cpu'\n",
    "n_training_samples = 100\n",
    "n_parameters = 2\n",
    "observation_key = \"x\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwOn4DGBPm4H"
   },
   "source": [
    "## Set input ... \n",
    "\n",
    "In order to make use of en external simulator called frrom the command line, the user must specify a function to setup the simulator input. It should take one input argument (the array with the input parameters), and return any input to be passed to the program via `stdin`. If the simulator requires any input files to be present, this function should write these to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "meSdtS2NFVLN"
   },
   "outputs": [],
   "source": [
    "def set_input(v):\n",
    "    v0 = v[0]\n",
    "    v1 = v[1]\n",
    "    v_str = str(v0).strip()+' '+str(v1).strip()\n",
    "    return v_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqeKigcaRA0F"
   },
   "source": [
    "##... output methods ...\n",
    "Analogously, the user must define a function to retrieve results from the simulator output. It should take two input arguments (stdout and stderr of the simulator run) and return a dictionary with the simulator output shaped as described by the ``sim_shapes`` argument. If the simulator writes output to disk, this function **should** parse the results from the file(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JOchVbJBFw4W"
   },
   "outputs": [],
   "source": [
    "def get_output(stdout,stderr):\n",
    "    try:\n",
    "        if not stderr :      \n",
    "            x0,x1 = stdout.split(\" \")\n",
    "            x0 = float(x0.strip())\n",
    "            x1 = float(x1.strip())\n",
    "            x = np.array([x0,x1])\n",
    "            return dict(x=x)\n",
    "\n",
    "        else:\n",
    "            raise('simulator returned on stderr')\n",
    "\n",
    "    except:\n",
    "        raise('Error in output retrieval')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ir01ri0TdTp"
   },
   "source": [
    "## ... and invocation\n",
    "\n",
    "Here we use the cell magic `%%writefile` command to create an external python function randgauss.py containing the simulator defined as model in the Quickstart notebook. This function is then invoked from the command line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QzzTXTQ0VOxw",
    "outputId": "12194a58-f0b9-4179-d413-df182deccbfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing randgauss.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile randgauss.py \n",
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "def rgmodel(v,sigma=0.05):\n",
    "    x = v + np.random.randn(2)*sigma\n",
    "    return x\n",
    "\n",
    "\n",
    "def main():\n",
    "    sigma = None\n",
    "    args = sys.stdin.readline()\n",
    "    arg1, arg2 = args.split(' ')\n",
    "    try:\n",
    "        v0 = float(arg1.rstrip())\n",
    "        v1 = float(arg2.rstrip())\n",
    "\n",
    "    except:\n",
    "        raise()\n",
    "\n",
    "    v = np.array([v0,v1])\n",
    "\n",
    "    if sigma is not None:\n",
    "        x = rgmodel(v,sigma=sigma)\n",
    "    else:\n",
    "        x = rgmodel(v)\n",
    "\n",
    "    print(str(x[0]).strip()+' '+str(x[1]).strip())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnLbuH2gVYaK"
   },
   "source": [
    "It is up to the user to ensure adaquate permissions for all relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zG4JzXJGHdjL"
   },
   "outputs": [],
   "source": [
    "!chmod 755 randgauss.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr7TL_K5U5tx"
   },
   "source": [
    "And to ensure that the root temporary directory in which the simulator is run exists.   Each instance of the simulator will run in a separate sub-folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GJ68rUrWVAox"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "L_Bzno0jVqcG"
   },
   "outputs": [],
   "source": [
    "command = cwd+'/randgauss.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neqLDwl_Yn3j"
   },
   "source": [
    "## Defining the Dask simulator\n",
    "The simulator itslef can then be defined using the `from_command()` method of the `DaskSimulator` class, exactly as for the `Simulator` class. A local dask cluster with a local scheduler is instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "esZz6kEvF0YJ"
   },
   "outputs": [],
   "source": [
    "simulator = swyft.DaskSimulator.from_command(\n",
    "    command=command,\n",
    "    parameter_names=[\"x0\",\"x1\"],\n",
    "    sim_shapes=dict(x=(n_parameters,)),\n",
    "    set_input_method=set_input,\n",
    "    get_output_method=get_output,\n",
    "    tmpdir=cwd+'/tmp/'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9lncfYIz36v"
   },
   "source": [
    "In order to connect to an external Dask cluster, and also locally for better performance, we make use of the Dask distributed scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RS-dxKB70Xrs"
   },
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster # from dask_jobqueue import SLURMCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0pvRoRo1lJu"
   },
   "source": [
    "Here we set the cluster the simulator will use, enabling the user to connect to a dask cluster of their choice.\n",
    "\n",
    "##Please Note\n",
    "When using the DaskSimulator class in combination with a simulator invoked from the command line, the user must ensure that it is available at the specified location on all nodes of the cluster.\n",
    "\n",
    "Furthermore, the number of threads per worker **must** be set to one, i.e. `threads_per_worker=1`. \n",
    "\n",
    "This is because each worker access a different scratch directory space, which can be achieved by process-parallelism only. The command-line simulator can then spawn multiple threads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fkGESBqD1d6c"
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=3,threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Cmw22Kr31-gQ"
   },
   "outputs": [],
   "source": [
    "simulator.set_dask_cluster(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86Yrdkfg2guT"
   },
   "source": [
    "## The directory store\n",
    "A directory store is initialized using the `Store.directory_store()` convenience function. The user must specify the path where to create a new store. If a store already exists, it can be opened using `Store.load()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UacXvEWiKjhe",
    "outputId": "ca4bf3b2-6234-42d1-a7b8-46bc61e031e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new store.\n"
     ]
    }
   ],
   "source": [
    "store = swyft.Store.directory_store(cwd+'/mystore', simulator=simulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3OAQLe13PZE"
   },
   "source": [
    "## Inferrence\n",
    "We can now define the intial prior, and given an observation, begin with the inferrence problem,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFGuEa1I4Tst",
    "outputId": "5c12ef79-4959-431c-e85f-cefec19643cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store: Adding 114 new samples to simulator store.\n"
     ]
    }
   ],
   "source": [
    "low = -1 * np.ones(n_parameters)\n",
    "high = 1 * np.ones(n_parameters)\n",
    "prior = swyft.get_uniform_prior(low, high)\n",
    "\n",
    "# drawing samples from the store is Poisson distributed. Simulating slightly more than we need avoids attempting to draw more than we have.\n",
    "store.add(n_training_samples + 0.01 * n_training_samples, prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLCRLzFQ4kvK"
   },
   "source": [
    "invoking the simulator to produce the required simulations. Here, we wait for the simmulations to complete (blocking the flow of execution) and then continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "P_3lDfAx4lVw"
   },
   "outputs": [],
   "source": [
    "store.simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FMXkjBE15RZ5"
   },
   "outputs": [],
   "source": [
    "dataset = swyft.Dataset(n_training_samples, prior, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    }
   ],
   "source": [
    "# The store / dataset is populated with samples simulated with dask.\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgrWXwtd6OSu"
   },
   "source": [
    "## Asynchronous simulation\n",
    "\n",
    "In some cases performing all the required simulations, even when parallelized, will take a significant amount of time. For such situations, the use of the `DaskSimulator` class, together with the directory store enables the asynchronous execution of the requested simulations. The dask workers then write the obtained simulation results directly to the store on disk, allowing the inference workflow to be suspened and later restarted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6Dm57gWgEZT"
   },
   "source": [
    "**Please note**\n",
    "\n",
    "For the example shown here, we have started the dask cluster from within the notebook. If the user expects simulations to take so much time as to warrant exiting and rejoining at a later time, then the Dask cluster should be started outside of the Jupyter notebook in order to keep working when the kernel stops. \n",
    "\n",
    "We refer the user to the [dask documentation](http://distributed.dask.org/en/stable/) and to [this example](https://pangeo.io/setup_guides/hpc.html#) for reference in using dask on an HPC system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "A0PuSCxX7bmk"
   },
   "outputs": [],
   "source": [
    "store.simulate(wait_for_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s320_NS8WOg"
   },
   "source": [
    "and when the simulations are complete, the (interrupted) workflow can continue as \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9NBnQq2cI0L",
    "outputId": "617b10e7-a0b8-40ff-b72a-50d6c6a7eddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing store.\n"
     ]
    }
   ],
   "source": [
    "rejoinedstore = swyft.Store.load(cwd+'/mystore', simulator=simulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZz1-2x68w9X",
    "outputId": "cfbfb79b-8552-4a38-993c-6aa09cb0d215"
   },
   "outputs": [],
   "source": [
    "dataset = swyft.Dataset(n_training_samples*8/10, prior, rejoinedstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "# The store / dataset is populated with samples simulated with dask.\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Examples - 6. Dask Simulator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
